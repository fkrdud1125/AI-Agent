import os
import io
import re
import json
from typing import Optional, List, Dict

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.ensemble import IsolationForest

# --- LLM: Gemini ê³µì‹ SDK ---
import google.generativeai as genai

# --------- ê¸°ë³¸ ì„¸íŒ… ----------
st.set_page_config(page_title="HARU", layout="wide")
st.markdown(
    "<h1 style='text-align: center; color: blue; font-size: 80px;'>H A R U</h1>",
    unsafe_allow_html=True
)

# --------- Gemini ì„¤ì • ----------
API_KEY = os.getenv("GOOGLE_API_KEY", "")
if not API_KEY:
    st.warning("í™˜ê²½ë³€ìˆ˜ GOOGLE_API_KEYê°€ ë¹„ì–´ìˆì–´ìš”. í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
genai.configure(api_key=API_KEY)

# ì§€ì› íŒŒì¼ í˜•ì‹ (CSV/TXT ì¶”ê°€)
ALLOWED_EXTS = {".xlsx", ".xls", ".csv", ".docx", ".pdf", ".txt"}

# ================== ìœ í‹¸/íŒŒì„œ ==================
def get_model(model_name: str, temperature: float = 0.2):
    return genai.GenerativeModel(
        model_name=model_name,
        generation_config={"temperature": temperature}
    )

def ask_llm(model_name: str, system_msg: str, user_msg: str, temperature: float = 0.2) -> str:
    model = get_model(model_name, temperature)
    parts = []
    if system_msg:
        parts.append(f"[SYSTEM]\n{system_msg}")
    parts.append(f"[USER]\n{user_msg}")
    resp = model.generate_content("\n\n".join(parts))
    try:
        return resp.text
    except Exception:
        return "(ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"

def read_excel(file_bytes: bytes) -> pd.DataFrame:
    return pd.read_excel(io.BytesIO(file_bytes))

def read_csv(file_bytes: bytes) -> pd.DataFrame:
    return pd.read_csv(io.BytesIO(file_bytes))

def read_word(file_bytes: bytes) -> str:
    from docx import Document
    doc = Document(io.BytesIO(file_bytes))
    return "\n".join([para.text for para in doc.paragraphs])

def read_pdf(file_bytes: bytes) -> str:
    from pypdf import PdfReader
    reader = PdfReader(io.BytesIO(file_bytes))
    return "\n".join([(p.extract_text() or "") for p in reader.pages])

def read_txt(file_bytes: bytes, encoding: str = "utf-8") -> str:
    try:
        return io.BytesIO(file_bytes).getvalue().decode(encoding)
    except UnicodeDecodeError:
        return io.BytesIO(file_bytes).getvalue().decode("cp949", errors="ignore")

def get_extension(filename: str) -> str:
    m = re.search(r"\.[A-Za-z0-9]+$", filename)
    return m.group(0).lower() if m else ""

# --------- í…ìŠ¤íŠ¸ EDA ----------
from collections import Counter

def text_eda_summary(text: str) -> dict:
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", text)
    return {
        "chars": len(text),
        "words": len(tokens),
        "unique_words": len(set(t.lower() for t in tokens)),
        "top_terms": Counter(t.lower() for t in tokens if len(t) >= 2).most_common(20),
    }

def render_text_eda(summary: dict):
    c1, c2, c3 = st.columns(3)
    c1.metric("ë¬¸ì ìˆ˜", f"{summary['chars']:,}")
    c2.metric("ë‹¨ì–´ ìˆ˜", f"{summary['words']:,}")
    c3.metric("ê³ ìœ  ë‹¨ì–´ ìˆ˜", f"{summary['unique_words']:,}")
    st.write("ìƒìœ„ ë‹¨ì–´(ìƒìœ„ 20)")
    st.dataframe(pd.DataFrame(summary["top_terms"], columns=["term", "freq"]), use_container_width=True)

# ================== EDA ==================

def dataframe_preview_metrics(df: pd.DataFrame):
    """í‘œ í˜•íƒœ ë¯¸ë¦¬ë³´ê¸°(head)ë¥¼ ì—†ì• ê³ , ë©”íŠ¸ë¦­/ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ìš”ì•½"""
    shape = df.shape
    with st.container():
        st.markdown("#### ğŸ“Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°(ìš”ì•½)")
        c1, c2, c3 = st.columns(3)
        c1.metric("í–‰(Row)", f"{shape[0]:,}")
        c2.metric("ì—´(Column)", f"{shape[1]:,}")
        c3.metric("ê²°ì¸¡ì¹˜ í¬í•¨ ì—´ ìˆ˜", f"{int((df.isna().any()).sum()):,}")

        st.markdown("**ì»¬ëŸ¼ ëª©ë¡(ìƒìœ„ 20ê°œê¹Œì§€)**")
        st.write(", ".join(list(df.columns)[:20]))

def dataframe_eda_retail(df: pd.DataFrame):
    """EDA"""
    # 3-1) ê²°ì¸¡ì¹˜/ê¸°ë³¸ í†µê³„
    missing = (df.isna().mean() * 100).round(2).sort_values(ascending=False)
    st.subheader("ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%)")
    st.dataframe(missing.to_frame("missing_%"), use_container_width=True)

    num_cols = df.select_dtypes(include="number").columns.tolist()
    if num_cols:
        st.subheader("ìˆ˜ì¹˜í˜• ê¸°ìˆ í†µê³„")
        st.dataframe(df[num_cols].describe().T, use_container_width=True)


    # 3-2) ë¹ ë¥¸ ì‹œê°í™”(ê°„ë‹¨)
    if num_cols:
        with st.expander("ğŸ“Š ë¹ ë¥¸ ì‹œê°í™”"):
            target = st.selectbox("íˆìŠ¤í† ê·¸ë¨ ëŒ€ìƒ", num_cols, key="hist_col")
            st.plotly_chart(px.histogram(df, x=target), use_container_width=True)
            if len(num_cols) >= 2:
                xcol = st.selectbox("ì‚°ì ë„ X", num_cols, key="xcol")
                ycol = st.selectbox("ì‚°ì ë„ Y", num_cols, key="ycol")
                st.plotly_chart(px.scatter(df, x=xcol, y=ycol), use_container_width=True)

    # 3-3) ë¡œì»¬ í…ìŠ¤íŠ¸ ì„¤ëª…
    st.subheader("ğŸ“„ ë¡œì»¬ EDA ì„¤ëª…")
    st.text(summarize_eda_local(df))

def summarize_eda_local(df: pd.DataFrame) -> str:
    total_rows, total_cols = df.shape
    missing_ratio = (df.isna().mean() * 100).round(2)
    num_cols = df.select_dtypes(include="number").columns.tolist()
    desc = df[num_cols].describe().T.round(2) if num_cols else None

    lines = []
    lines.append(f"ì´ í–‰ ìˆ˜: {total_rows} / ì—´ ìˆ˜: {total_cols}")
    if not missing_ratio.empty:
        lines.append("ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ìƒìœ„ 5ê°œ ì—´:")
        lines.extend([f"- {col}: {missing_ratio[col]}%" for col in missing_ratio.head(5).index])
    if desc is not None and not desc.empty:
        lines.append("ë³€ë™ì„±ì´ í° ì—´ Top 3:")
        var_top = desc.sort_values("std", ascending=False).head(3).index.tolist()
        lines.extend([f"- {col}" for col in var_top])
    return "\n".join(lines)

# ================== ì´ìƒíƒì§€ ==================
def detect_anomalies(df: pd.DataFrame, cols: List[str], contamination: float = 0.03, n_estimators: int = 300) -> Optional[pd.DataFrame]:
    if not cols:
        st.warning("ì´ìƒíƒì§€ì— ì‚¬ìš©í•  ì—´ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        return None
    sub = df[cols].dropna()
    if sub.shape[0] < 10:
        st.warning("ì´ìƒíƒì§€ì— ì¶©ë¶„í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤(ìµœì†Œ 10í–‰ ê¶Œì¥).")
        return None
    iso = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=42)
    labels = iso.fit_predict(sub.values)  # -1: anomaly, 1: normal
    scores = iso.score_samples(sub.values)  # ì ìˆ˜(ì‘ì„ìˆ˜ë¡ ì´ìƒ)
    res = df.copy()
    res["_anomaly"] = "normal"
    res.loc[sub.index[labels == -1], "_anomaly"] = "anomaly"
    res.loc[sub.index, "_anomaly_score"] = scores
    return res

def summarize_anomalies_local(res: pd.DataFrame, used_cols: List[str]) -> str:
    if "_anomaly" not in res:
        return "ì´ìƒíƒì§€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    total = len(res)
    num_anom = (res["_anomaly"] == "anomaly").sum()
    ratio = round(num_anom / total * 100, 2)
    lines = [f"ì „ì²´ {total}í–‰ ì¤‘ ì´ìƒì¹˜ {num_anom}ê°œ ({ratio}%) ë°œê²¬",
             f"ì‚¬ìš© ì—´: {', '.join(used_cols) if used_cols else '(ì„ íƒ ì—†ìŒ)'}"]
    anoms = res[res["_anomaly"]=="anomaly"].copy()
    if not anoms.empty:
        if "_anomaly_score" in anoms:
            anoms = anoms.sort_values("_anomaly_score").head(5)
        else:
            anoms = anoms.head(5)
        show_cols = ["_anomaly_score"] + [c for c in used_cols if c in res.columns][:5]
        lines.append("ìƒìœ„ ì´ìƒì¹˜ ë¯¸ë¦¬ë³´ê¸°:")
        lines.append(str(anoms[show_cols].round(4)))
    return "\n".join(lines)

# ================== ìš”ì•½ JSON(ì§ˆë¬¸ìš©) ==================
def build_summary_for_llm(df: pd.DataFrame, res: Optional[pd.DataFrame], used_cols: Optional[List[str]] = None) -> str:
    summary = {
        "shape": {"rows": len(df), "cols": len(df.columns)},
        "columns": list(df.columns),
        "missing_ratio": (df.isna().mean() * 100).round(2).to_dict(),
    }
    num = df.select_dtypes(include="number")
    if not num.empty:
        summary["describe"] = num.describe().T.round(2).to_dict()
        corr = num.corr(numeric_only=True).abs().unstack().sort_values(ascending=False)
        corr = corr[corr.index.get_level_values(0) < corr.index.get_level_values(1)]
        summary["top_corr_pairs"] = [{"pair": f"{a}~{b}", "corr": round(float(v),4)} for (a,b), v in corr.head(8).items()]
    if isinstance(res, pd.DataFrame) and "_anomaly" in res:
        total = len(res)
        num_anom = int((res["_anomaly"] == "anomaly").sum())
        summary["anomaly"] = {
            "count": num_anom,
            "ratio_pct": round((num_anom / total * 100) if total else 0.0, 2),
            "used_cols": used_cols or num.columns.tolist()
        }
        anoms = res[res["_anomaly"] == "anomaly"].copy()
        if "_anomaly_score" in anoms:
            anoms = anoms.sort_values("_anomaly_score").head(20)
        else:
            anoms = anoms.head(20)
        num_cols = num.columns.tolist()
        cols_show = (["_anomaly_score"] if "_anomaly_score" in anoms.columns else []) + (used_cols or num_cols)[:6]
        summary["top_anomalies_preview"] = anoms[cols_show].round(4).reset_index().to_dict(orient="records")
    return json.dumps(summary, ensure_ascii=False)

def build_context_from_file(ext: str, df: Optional[pd.DataFrame], text: Optional[str]) -> str:
    if ext in {".docx", ".pdf", ".txt"} and text is not None:
        s = text.strip()
        s_short = s[:4000]
        eda = text_eda_summary(s)
        top = ", ".join([f"{t}({c})" for t, c in eda["top_terms"][:10]])
        return f"ë‹¤ìŒì€ ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì¼ë¶€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:\n{s_short}\n\nìƒìœ„ ë‹¨ì–´: {top}\n"
    else:
        return "ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

# ================== UI ==================

# 1) LLM ëª¨ë¸ ì„ íƒì°½
st.subheader("1) LLM ëª¨ë¸ ì„ íƒ")
model_name = st.selectbox("Gemini ëª¨ë¸", ["gemini-1.5-flash", "gemini-1.5-pro"], index=0)
temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.05)

st.divider()

# 2) íŒŒì¼ ì‚½ì…
st.subheader("2) íŒŒì¼ ì‚½ì… (Excel / CSV / Word / PDF / TXT)")
up = st.file_uploader("Upload your file first", type=list({e.strip('.') for e in ALLOWED_EXTS}))

ext = None
df: Optional[pd.DataFrame] = None
doc_text: Optional[str] = None

if up is not None:
    ext = get_extension(up.name)
    if ext not in ALLOWED_EXTS:
        st.error("ì˜³ì§€ ì•Šì€ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. Excel(.xlsx/.xls), CSV(.csv), Word(.docx), PDF(.pdf), TXT(.txt)ë§Œ ì§€ì›í•´ìš”.")
        st.stop()

    data = up.read()

    try:
        if ext in {".xlsx", ".xls"}:
            df = read_excel(data)
            st.success(f"ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
        elif ext == ".csv":
            df = read_csv(data)
            st.success(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
        elif ext == ".docx":
            doc_text = read_word(data)
            st.success("Word íŒŒì¼ ë¡œë“œ ì„±ê³µ")
        elif ext == ".pdf":
            doc_text = read_pdf(data)
            st.success("PDF íŒŒì¼ ë¡œë“œ ì„±ê³µ")
        elif ext == ".txt":
            doc_text = read_txt(data)
            st.success("TXT íŒŒì¼ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        st.error(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")

    # 2) ì¶”ì¶œ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
    st.subheader("ğŸ“„ ì¶”ì¶œ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°")
    if df is not None:
        schema = {c: str(df[c].dtype) for c in df.columns}
        preview = {"columns": df.columns.tolist(), "dtypes": schema}
        st.text_area("í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ(JSON)", json.dumps(preview, ensure_ascii=False, indent=2), height=200)
        # 2-1) ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°(ë©”íŠ¸ë¦­í˜•) â€” í‘œ head ì œê±°
        dataframe_preview_metrics(df)
        # ì„¸ì…˜ ë³´ì¡´
        st.session_state["df"] = df
    elif doc_text is not None:
        st.text_area("ë¬¸ì„œ ë³¸ë¬¸ ì¼ë¶€", doc_text[:5000], height=200)
        render_text_eda(text_eda_summary(doc_text))
        st.session_state["doc_text"] = doc_text
    else:
        st.info("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

st.divider()

# 3) EDA
st.subheader("3) EDA")
df = st.session_state.get("df", df)
doc_text = st.session_state.get("doc_text", doc_text)
if df is not None:
    dataframe_eda_retail(df)
elif doc_text is not None:
    st.info("ë¬¸ì„œí˜• íŒŒì¼ì€ ìƒë‹¨ 'ì¶”ì¶œ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°'ì—ì„œ EDA ì§€í‘œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
else:
    st.info("Upload your file first")

st.divider()

# 4) ì´ìƒë¶„ì„ (í† ê¸€ ì—†ì´ ëª…ì‹œ ë²„íŠ¼)
st.subheader("4) ì´ìƒë¶„ì„")
if df is not None:
    num_cols_all = df.select_dtypes(include="number").columns.tolist()
    if num_cols_all:
        c1, c2 = st.columns([2, 1])
        with c1:
            sel_cols = st.multiselect("ì´ìƒíƒì§€ì— ì‚¬ìš©í•  ìˆ˜ì¹˜í˜• ì—´ì„ ì„ íƒí•˜ì„¸ìš”", num_cols_all, key="anom_cols_sel")
        with c2:
            contamination = st.slider("ì˜¤ì—¼ë„(ì´ìƒ ë¹„ìœ¨ ì¶”ì •)", 0.01, 0.2, 0.03, 0.01, key="anom_cont")
        run = st.button("ğŸš¨ ì´ìƒíƒì§€ ì‹¤í–‰", use_container_width=True)
        if run:
            res = detect_anomalies(df, sel_cols, contamination=contamination)
            if res is not None:
                st.session_state["res"] = res
                st.session_state["anom_cols"] = sel_cols
                st.success(f"ì´ìƒíƒì§€ ì™„ë£Œ: ì´ìƒì¹˜ {(res['_anomaly']=='anomaly').sum()}ê±´")
                # ê²°ê³¼ í…Œì´ë¸” ë° ê°„ë‹¨ ì‹œê°í™”
                st.dataframe(res, use_container_width=True)
                if len(sel_cols) == 1:
                    tmp = res[[sel_cols[0], "_anomaly"]].copy()
                    tmp["_idx"] = range(len(tmp))
                    st.plotly_chart(px.scatter(tmp, x="_idx", y=sel_cols[0], color="_anomaly"), use_container_width=True)
                # ë¡œì»¬ í…ìŠ¤íŠ¸ ì„¤ëª…
                st.markdown("### ğŸ“„ ë¡œì»¬ ì´ìƒíƒì§€ ì„¤ëª…")
                st.text(summarize_anomalies_local(res, sel_cols))
    else:
        st.info("ìˆ˜ì¹˜í˜• ì—´ì´ ì—†ìŠµë‹ˆë‹¤. ì´ìƒíƒì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
elif doc_text is not None:
    st.info("ë¬¸ì„œí˜• íŒŒì¼ì€ ì´ìƒíƒì§€ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
else:
    st.info("Upload your file first")

st.divider()

# 5) ì§ˆë¬¸í•˜ê¸° (LLM ì‘ë‹µ) â€” ìš”ì•½ë³¸ë§Œ ì „ì†¡
st.subheader("5) ì§ˆë¬¸í•˜ê¸° (LLM ì‘ë‹µ)")
user_q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="Ask a Question here")
if st.button("ì§ˆë¬¸ ì „ì†¡", use_container_width=True) and user_q.strip():
    df = st.session_state.get("df", df)
    res = st.session_state.get("res", None)
    used_cols = st.session_state.get("anom_cols", None)

    if df is not None:
        summary_json = build_summary_for_llm(df, res, used_cols)
        system_prompt = (
            "ë„ˆëŠ” ì—…ë¡œë“œëœ ë°ì´í„° ìš”ì•½ë³¸(JSON)ì„ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ê·¼ê±° ìˆëŠ” ë¶„ì„ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì•¼. "
            "JSONì˜ ìˆ˜ì¹˜ì™€ í†µê³„ë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•´. JSON ë°– ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë‹µí•´."
        )
        user_prompt = f"[ìš”ì•½ JSON]\n{summary_json}\n\n[ì§ˆë¬¸]\n{user_q}"
    elif doc_text is not None:
        ctx = build_context_from_file(ext or "", None, doc_text)
        system_prompt = "ë„ˆëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì•¼. ì»¨í…ìŠ¤íŠ¸ ë°–ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆ."
        user_prompt = f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n\n[ì§ˆë¬¸]\n{user_q}"
    else:
        st.warning("Upload your file first")
        st.stop()

    with st.spinner("ìƒê° ì¤‘..."):
        answer = ask_llm(model_name, system_prompt, user_prompt, temperature=temperature)
    st.markdown("### ğŸ§  LLM ì‘ë‹µ")
    st.markdown(answer)
