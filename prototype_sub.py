import os
import io
import re
import json
from typing import Optional, List
import streamlit as st
import pandas as pd
import plotly.express as px
from sklearn.ensemble import IsolationForest

# --- LLM: Gemini ê³µì‹ SDK ---
import google.generativeai as genai

# --------- ê¸°ë³¸ ì„¸íŒ… ----------
st.set_page_config(page_title="HARU AI Portal", layout="wide")
st.title("ğŸ¤– HARU AI Portal (Gemini)")

# --------- Gemini ì„¤ì • ----------
API_KEY = os.getenv("GOOGLE_API_KEY", "")
if not API_KEY:
    st.warning("í™˜ê²½ë³€ìˆ˜ GOOGLE_API_KEYê°€ ë¹„ì–´ìˆì–´ìš”. í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
genai.configure(api_key=API_KEY)

# ì§€ì› íŒŒì¼ í˜•ì‹ (CSV/TXT ì¶”ê°€)
ALLOWED_EXTS = {".xlsx", ".xls", ".csv", ".docx", ".pdf", ".txt"}

# --------- LLM ìœ í‹¸ ----------
def get_model(model_name: str, temperature: float = 0.2):
    return genai.GenerativeModel(
        model_name=model_name,
        generation_config={"temperature": temperature}
    )

def ask_llm(model_name: str, system_msg: str, user_msg: str, temperature: float = 0.2) -> str:
    model = get_model(model_name, temperature)
    parts = []
    if system_msg:
        parts.append(f"[SYSTEM]\n{system_msg}")
    parts.append(f"[USER]\n{user_msg}")
    resp = model.generate_content("\n\n".join(parts))
    try:
        return resp.text
    except Exception:
        return "(ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"

# --------- íŒŒì¼ íŒŒì„œ ----------
def read_excel(file_bytes: bytes) -> pd.DataFrame:
    return pd.read_excel(io.BytesIO(file_bytes))

def read_csv(file_bytes: bytes) -> pd.DataFrame:
    return pd.read_csv(io.BytesIO(file_bytes))

def read_word(file_bytes: bytes) -> str:
    from docx import Document
    doc = Document(io.BytesIO(file_bytes))
    return "\n".join([para.text for para in doc.paragraphs])

def read_pdf(file_bytes: bytes) -> str:
    from pypdf import PdfReader
    reader = PdfReader(io.BytesIO(file_bytes))
    return "\n".join([(p.extract_text() or "") for p in reader.pages])

def read_txt(file_bytes: bytes, encoding: str = "utf-8") -> str:
    try:
        return io.BytesIO(file_bytes).getvalue().decode(encoding)
    except UnicodeDecodeError:
        # í•œê¸€ íŒŒì¼ì—ì„œ ê°„í˜¹ cp949 ë“± í•„ìš”í•  ë•Œ
        return io.BytesIO(file_bytes).getvalue().decode("cp949", errors="ignore")

def get_extension(filename: str) -> str:
    m = re.search(r"\.[A-Za-z0-9]+$", filename)
    return m.group(0).lower() if m else ""

# --------- í…ìŠ¤íŠ¸ EDA ----------
from collections import Counter
def text_eda_summary(text: str) -> dict:
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", text)
    word_count = len(tokens)
    char_count = len(text)
    unique_count = len(set(t.lower() for t in tokens))
    top_terms = Counter(t.lower() for t in tokens if len(t) >= 2).most_common(20)
    return {"chars": char_count, "words": word_count, "unique_words": unique_count, "top_terms": top_terms}

def render_text_eda(summary: dict):
    c1, c2, c3 = st.columns(3)
    c1.metric("ë¬¸ì ìˆ˜", f"{summary['chars']:,}")
    c2.metric("ë‹¨ì–´ ìˆ˜", f"{summary['words']:,}")
    c3.metric("ê³ ìœ  ë‹¨ì–´ ìˆ˜", f"{summary['unique_words']:,}")
    st.write("ìƒìœ„ ë‹¨ì–´(ìƒìœ„ 20)")
    st.dataframe(pd.DataFrame(summary["top_terms"], columns=["term", "freq"]), use_container_width=True)

# --------- ë°ì´í„°í”„ë ˆì„ EDA ----------
def dataframe_eda(df: pd.DataFrame):
    st.write("ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°")
    st.dataframe(df.head(10), use_container_width=True)

    st.write("ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%)")
    missing = (df.isna().mean() * 100).round(2)
    st.dataframe(missing.to_frame("missing_%"), use_container_width=True)

    numeric_cols = df.select_dtypes(include="number").columns.tolist()
    if numeric_cols:
        st.write("ìˆ˜ì¹˜í˜• ê¸°ìˆ í†µê³„")
        st.dataframe(df.describe().T, use_container_width=True)

        with st.expander("ğŸ“Š ë¹ ë¥¸ ì‹œê°í™”"):
            target = st.selectbox("íˆìŠ¤í† ê·¸ë¨ ëŒ€ìƒ", numeric_cols, key="hist_col")
            fig = px.histogram(df, x=target)
            st.plotly_chart(fig, use_container_width=True)

            if len(numeric_cols) >= 2:
                xcol = st.selectbox("ì‚°ì ë„ X", numeric_cols, key="xcol")
                ycol = st.selectbox("ì‚°ì ë„ Y", numeric_cols, key="ycol")
                fig2 = px.scatter(df, x=xcol, y=ycol)
                st.plotly_chart(fig2, use_container_width=True)
    else:
        st.info("ìˆ˜ì¹˜í˜• ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")

# --------- ì´ìƒíƒì§€ ----------
def detect_anomalies(df: pd.DataFrame, cols: List[str], contamination: float = 0.03, n_estimators: int = 300) -> Optional[pd.DataFrame]:
    if not cols:
        st.warning("ì´ìƒíƒì§€ì— ì‚¬ìš©í•  ìˆ˜ì¹˜í˜• ì—´ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        return None
    sub = df[cols].dropna()
    if sub.shape[0] < 10:
        st.warning("ì´ìƒíƒì§€ì— ì¶©ë¶„í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤(ìµœì†Œ 10í–‰ ê¶Œì¥).")
        return None
    iso = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=42)
    lab = iso.fit_predict(sub.values)  # -1: anomaly, 1: normal
    res = df.copy()
    res["_anomaly"] = "normal"
    res.loc[sub.index[lab == -1], "_anomaly"] = "anomaly"
    return res

# --------- LLM ì»¨í…ìŠ¤íŠ¸ ----------
def build_context_from_file(ext: str, df: Optional[pd.DataFrame], text: Optional[str]) -> str:
    """
    ì§ˆë¬¸ ì‹œ LLMì— ë„˜ê¸¸ ì»¨í…ìŠ¤íŠ¸(ìš”ì•½ ìŠ¤ëƒ…ìƒ·).
    - Excel/CSV: ì¹¼ëŸ¼, dtype, head(5)
    - Word/PDF/TXT: ë³¸ë¬¸ ì¼ë¶€ + ìƒìœ„ ë‹¨ì–´
    """
    if ext in {".xlsx", ".xls", ".csv"} and df is not None:
        snap = {
            "columns": df.columns.tolist(),
            "dtypes": {c: str(df[c].dtype) for c in df.columns},
            "head": df.head(5).to_dict(orient="records")
        }
        return "ë‹¤ìŒì€ ì—…ë¡œë“œëœ í…Œì´ë¸” ë°ì´í„°(Excel/CSV)ì˜ ìŠ¤ëƒ…ìƒ·ì…ë‹ˆë‹¤.\n" + json.dumps(snap, ensure_ascii=False)
    elif ext in {".docx", ".pdf", ".txt"} and text is not None:
        s = text.strip()
        s_short = s[:4000]
        eda = text_eda_summary(s)
        top = ", ".join([f"{t}({c})" for t, c in eda["top_terms"][:10]])
        return f"ë‹¤ìŒì€ ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì¼ë¶€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:\n{s_short}\n\nìƒìœ„ ë‹¨ì–´: {top}\n"
    else:
        return "ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

# ================== UI ==================

# 1) LLM ëª¨ë¸ ì„ íƒì°½
st.subheader("1) LLM ëª¨ë¸ ì„ íƒ")
model_name = st.selectbox("Gemini ëª¨ë¸", ["gemini-1.5-flash", "gemini-1.5-pro"], index=0)
temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.05)

st.divider()

# 2) íŒŒì¼ ì‚½ì… (CSV/TXT ì¶”ê°€)
st.subheader("2) íŒŒì¼ ì‚½ì… (Excel / CSV / Word / PDF / TXT)")
up = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=list({e.strip('.') for e in ALLOWED_EXTS}))

ext = None
df: Optional[pd.DataFrame] = None
doc_text: Optional[str] = None

if up is not None:
    ext = get_extension(up.name)
    if ext not in ALLOWED_EXTS:
        st.error("ì˜³ì§€ ì•Šì€ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. Excel(.xlsx/.xls), CSV(.csv), Word(.docx), PDF(.pdf), TXT(.txt)ë§Œ ì§€ì›í•´ìš”.")
        st.stop()

    data = up.read()

    try:
        if ext in {".xlsx", ".xls"}:
            df = read_excel(data)
            st.success(f"ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
        elif ext == ".csv":
            df = read_csv(data)
            st.success(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
        elif ext == ".docx":
            doc_text = read_word(data)
            st.success("ì›Œë“œ íŒŒì¼ ë¡œë“œ ì„±ê³µ")
        elif ext == ".pdf":
            doc_text = read_pdf(data)
            st.success("PDF íŒŒì¼ ë¡œë“œ ì„±ê³µ")
        elif ext == ".txt":
            doc_text = read_txt(data)
            st.success("TXT íŒŒì¼ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        st.error(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")

    # ===== (ì‹ ê·œ) ì—…ë¡œë“œ ì§í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¯¸ë¦¬ë³´ê¸° =====
    st.subheader("ğŸ“„ ì¶”ì¶œ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°")
    if df is not None:
        # í‘œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œë„ ë³´ì—¬ì£¼ê¸°: ìŠ¤í‚¤ë§ˆ + headë¥¼ JSON ë¬¸ìì—´ë¡œ
        schema = {c: str(df[c].dtype) for c in df.columns}
        preview = {
            "columns": df.columns.tolist(),
            "dtypes": schema,
            "head": df.head(5).to_dict(orient="records"),
        }
        st.text_area("í…Œì´ë¸” ìŠ¤ëƒ…ìƒ·(JSON)", json.dumps(preview, ensure_ascii=False, indent=2), height=220)
        st.caption("â€» ì•„ë˜ì—ì„œ EDA/ì´ìƒíƒì§€ë¥¼ ê³„ì† ì§„í–‰í•˜ì„¸ìš”.")
    elif doc_text is not None:
        st.text_area("ë¬¸ì„œ ë³¸ë¬¸ ì¼ë¶€", doc_text[:5000], height=220)
        st.caption("â€» ì•„ë˜ì—ì„œ í…ìŠ¤íŠ¸ EDAë¥¼ ê³„ì† ì§„í–‰í•˜ì„¸ìš”.")
    else:
        st.info("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

st.divider()

# 3) íŒŒì¼ EDAì™€ ì´ìƒë¶„ì„
st.subheader("3) íŒŒì¼ EDAì™€ ì´ìƒë¶„ì„")
if df is not None:
    dataframe_eda(df)
    with st.expander("ğŸš¨ ì´ìƒíƒì§€ (IsolationForest)"):
        num_cols = df.select_dtypes(include="number").columns.tolist()
        if num_cols:
            sel = st.multiselect("ì´ìƒíƒì§€ì— ì‚¬ìš©í•  ì—´", num_cols)
            contamination = st.slider("ì˜¤ì—¼ë„(ì´ìƒ ë¹„ìœ¨ ì¶”ì •)", 0.01, 0.2, 0.03, 0.01)
            if st.button("ì´ìƒíƒì§€ ì‹¤í–‰"):
                res = detect_anomalies(df, sel, contamination=contamination)
                if res is not None:
                    st.dataframe(res, use_container_width=True)
                    if len(sel) == 1:
                        tmp = res[[sel[0], "_anomaly"]].copy()
                        tmp["_idx"] = range(len(tmp))
                        fig = px.scatter(tmp, x="_idx", y=sel[0], color="_anomaly")
                        st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ìˆ˜ì¹˜í˜• ì—´ì´ ì—†ìŠµë‹ˆë‹¤. ì´ìƒíƒì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
elif doc_text is not None:
    st.write("ë¬¸ì„œ EDA (í…ìŠ¤íŠ¸ ê¸°ë°˜)")
    eda = text_eda_summary(doc_text)
    render_text_eda(eda)
else:
    st.info("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. (Excel/CSV/Word/PDF/TXT)")

st.divider()

# 4) ì§ˆë¬¸í•˜ê¸° (LLM ì‘ë‹µ)
st.subheader("4) ì§ˆë¬¸í•˜ê¸° (LLM ì‘ë‹µ)")
user_q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", ...)
if st.button("ì§ˆë¬¸ ì „ì†¡", use_container_width=True) and user_q.strip():
    ctx = build_context_from_file(ext or "", df, doc_text)
    system_prompt = (...)
    user_prompt = f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx}\n\n[ì§ˆë¬¸]\n{user_q}"
    with st.spinner("ìƒê° ì¤‘..."):
        answer = ask_llm(...)
